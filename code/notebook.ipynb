{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26f68dc",
   "metadata": {},
   "source": [
    "# A Watermark for Large Language Models\n",
    "\n",
    "This is the Python notebook for our project. It generally follows the flow of the original paper, A Watermark for Large Language Models (Kirchenbauer et al. 2023). Much of the text and images in each section is pulled directly from the paper, though we have appended our own code to implement the paper's watermarking strategy and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install block - put any necessary pip installs here\n",
    "!pip install datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fa5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block - put any necessary imports here\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList\n",
    "from functools import partial\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329c439",
   "metadata": {},
   "source": [
    "# Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(F\"Device set to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a181ca0",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "The paper uses the C4 datasetâ€™s RealNewsLike subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"c4\"\n",
    "dataset_config_name = \"realnewslike\"\n",
    "dataset = load_dataset(dataset_name, dataset_config_name, split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705d6d6",
   "metadata": {},
   "source": [
    "# Load the Opt-1.3b tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Helpers\n",
    "def tokenize(sequence):\n",
    "    return tokenizer(sequence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "def seed_rng(seed=42):\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c300f",
   "metadata": {},
   "source": [
    "# Algorithm 1: Text Generation with Hard Red List\n",
    "\n",
    "![images/algorithm_1.png](images/algorithm_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f3a62",
   "metadata": {},
   "source": [
    "## Define the Hard Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardRedListWatermark(LogitsProcessor):\n",
    "\n",
    "    def __init__(self, tokenizer, device, hash_key=15485863):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.green_list_size = self.vocab_size // 2\n",
    "        # Large prime number to be used for seed\n",
    "        self.hash_key = hash_key\n",
    "        self.generator = torch.Generator(device=device)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Compute hash of previous token and set it as seed\n",
    "        prev_token = int(input_ids[0, -1].item())\n",
    "        self.generator.manual_seed(self.hash_key * prev_token)\n",
    "\n",
    "        # Shuffle the vocabulary and get red list ids\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=self.generator, device=input_ids.device)\n",
    "        red_list = permuted_vocab[self.green_list_size:] # continue where green list left off\n",
    "\n",
    "        # Set red list logits to -infinity\n",
    "        scores[:, red_list] = -float(\"inf\")\n",
    "        return scores\n",
    "\n",
    "    def compute_green_and_red_lists(self):\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=self.generator, device=self.device)\n",
    "        green_list = permuted_vocab[:self.green_list_size]\n",
    "        red_list = permuted_vocab[self.green_list_size:]\n",
    "\n",
    "        return green_list, red_list\n",
    "\n",
    "    def produce_color_map(self, prompt_ids, generated_token_ids):\n",
    "        # Set seed for reproducibility\n",
    "        seed_rng()\n",
    "\n",
    "        # Get last prompt_id\n",
    "        prev_token_id = int(prompt_ids['input_ids'][0, -1].item())\n",
    "\n",
    "        color_map = []\n",
    "\n",
    "        for i, token_id in enumerate(generated_token_ids['input_ids'][0]):\n",
    "            token_id = int(token_id.item())\n",
    "            self.generator.manual_seed(self.hash_key * prev_token_id)\n",
    "\n",
    "            green_list, _ = self.compute_green_and_red_lists()\n",
    "\n",
    "            if token_id in green_list:\n",
    "                color_map.append(1) # green\n",
    "            else:\n",
    "                color_map.append(0) # red\n",
    "\n",
    "            # Update previous token before next iteration\n",
    "            prev_token_id = token_id\n",
    "\n",
    "        return color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3aefa",
   "metadata": {},
   "source": [
    "## Implement Algorithm 1 using Hard Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 2.0\n",
    "gamma = 0.5\n",
    "\n",
    "hard_watermark = HardRedListWatermark(tokenizer, model.device)\n",
    "\n",
    "def algorithm_1(tokenizer, model, prompt_ids):\n",
    "    # Set seed for reproducibility\n",
    "    seed_rng()\n",
    "\n",
    "    # Instantiate the hard red list logits processor\n",
    "    hard_red_list_lp = LogitsProcessorList([hard_watermark])\n",
    "\n",
    "    # Generate using the hard red list logits processor\n",
    "    algorithm_1_generate = partial(\n",
    "        model.generate,\n",
    "        logits_processor=hard_red_list_lp,\n",
    "        max_new_tokens=200,\n",
    "        min_new_tokens=1,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Generate output ids\n",
    "    output_ids = algorithm_1_generate(**prompt_ids)\n",
    "\n",
    "    # Decode and return the string\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a71787",
   "metadata": {},
   "source": [
    "## Running Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272dc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def compute_z_score(green_token_count, total_length, gamma):\n",
    "    numerator = green_token_count - (total_length / 2)\n",
    "    denominator = math.sqrt(total_length)\n",
    "    return 2 * numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, prompts in enumerate(dataset):\n",
    "  prompt = prompts[\"text\"]\n",
    "  # Tokenize prompt into ids\n",
    "  prompt_ids = tokenize(prompt)\n",
    "  # Generate output\n",
    "  output = algorithm_1(tokenizer, model, prompt_ids)\n",
    "  continuation = output[len(prompt):]\n",
    "  generated_token_ids = tokenize(continuation)\n",
    "  # Given a continuation, compute which tokens are in the red list and which are in the green list\n",
    "  color_map = hard_watermark.produce_color_map(\n",
    "    prompt_ids = prompt_ids,\n",
    "    generated_token_ids = generated_token_ids,\n",
    "  )\n",
    "  continuation_token_count = len(generated_token_ids['input_ids'][0])\n",
    "  green_token_count = len(list(filter(lambda x: x, color_map)))\n",
    "\n",
    "  # Collect results\n",
    "  result_dict = {\n",
    "    \"prompt\": prompt,\n",
    "    \"continuation\": continuation,\n",
    "    \"full_output\": output,\n",
    "    \"continuation_token_count\": continuation_token_count,\n",
    "    \"green_token_count\": green_token_count,\n",
    "    \"z_score\": compute_z_score(green_token_count, continuation_token_count, gamma)\n",
    "  }\n",
    "  results.append(result_dict)\n",
    "  print(json.dumps(result_dict, indent=1))\n",
    "\n",
    "  # For now just break after generating 1 result since it takes a while\n",
    "  if i >= 0:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2986d",
   "metadata": {},
   "source": [
    "# Algorithm 2: Text Generation with Soft Red List\n",
    "\n",
    "![images/algorithm_2.png](images/algorithm_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f706aa3",
   "metadata": {},
   "source": [
    "## Define the Soft Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbff596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftRedListWatermark(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, device, gamma=0.5, hash_key=15485863, delta=2.0):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.gamma = gamma\n",
    "        self.hash_key = hash_key\n",
    "        self.delta = delta\n",
    "        self.green_list_size = int(self.gamma * self.vocab_size)\n",
    "        self.generator = torch.Generator(device=device)\n",
    "\n",
    "        # Entropy metrics\n",
    "        self.spike_entropies = []\n",
    "        self.z_parameter = (1 - gamma) * (math.exp(delta) - 1) / (1 + (math.exp(delta) - 1) * gamma)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Get raw probabilities before watermarking\n",
    "        raw_probs = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Compute spike entropy of the raw distribution\n",
    "        entropy = self.compute_spike_entropy(raw_probs, self.z_parameter)\n",
    "        self.spike_entropies.append(entropy.item())\n",
    "\n",
    "        # Compute hash of previous token and set it as seed\n",
    "        prev_token = int(input_ids[0, -1].item())\n",
    "        self.generator.manual_seed(self.hash_key * prev_token)\n",
    "\n",
    "        # Shuffle the vocabulary and get green list ids\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=self.generator, device=input_ids.device)\n",
    "        green_list = permuted_vocab[:self.green_list_size]\n",
    "\n",
    "        # Add delta to green list logits\n",
    "        scores[:, green_list] += self.delta\n",
    "        return scores\n",
    "\n",
    "    def compute_spike_entropy(self, p, z):\n",
    "        \"\"\"Calculate spike entropy of probability vector p with modulus z.\"\"\"\n",
    "        denom = 1.0 + z * p\n",
    "        denom = torch.clamp(denom, min=1e-9)\n",
    "        return torch.sum(p / denom)\n",
    "\n",
    "    def get_average_spike_entropy(self):\n",
    "        \"\"\"Calculate the average spike entropy over the generated sequence.\"\"\"\n",
    "        if not self.spike_entropies:\n",
    "            return 0.0\n",
    "        return sum(self.spike_entropies) / len(self.spike_entropies)\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset the spike entropy measurements for a new generation.\"\"\"\n",
    "        self.spike_entropies = []\n",
    "\n",
    "    def compute_green_and_red_lists(self):\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=self.generator, device=self.device)\n",
    "        green_list = permuted_vocab[:self.green_list_size]\n",
    "        red_list = permuted_vocab[self.green_list_size:]\n",
    "\n",
    "        return green_list, red_list\n",
    "\n",
    "    def produce_color_map(self, prompt_ids, generated_token_ids):\n",
    "        # Set seed for reproducibility\n",
    "        seed_rng()\n",
    "\n",
    "        # Get last prompt_id\n",
    "        prev_token_id = int(prompt_ids['input_ids'][0, -1].item())\n",
    "\n",
    "        color_map = []\n",
    "\n",
    "        for i, token_id in enumerate(generated_token_ids['input_ids'][0]):\n",
    "            token_id = int(token_id.item())\n",
    "            self.generator.manual_seed(self.hash_key * prev_token_id)\n",
    "\n",
    "            green_list, _ = self.compute_green_and_red_lists()\n",
    "\n",
    "            if token_id in green_list:\n",
    "                color_map.append(1) # green\n",
    "            else:\n",
    "                color_map.append(0) # red\n",
    "\n",
    "            # Update previous token before next iteration\n",
    "            prev_token_id = token_id\n",
    "\n",
    "        return color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf1921",
   "metadata": {},
   "source": [
    "## Implement Algorithm 2 using Soft Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 2.0\n",
    "gamma = 0.5\n",
    "\n",
    "soft_watermark = SoftRedListWatermark(tokenizer, model.device)\n",
    "\n",
    "def algorithm_2(tokenizer, model, prompt_ids):\n",
    "    # Set seed for reproducibility\n",
    "    seed_rng()\n",
    "\n",
    "    # Generate without watermark\n",
    "    output_no_watermark = model.generate(\n",
    "        **prompt_ids,\n",
    "        max_new_tokens=200,\n",
    "        min_new_tokens=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    text_no_watermark = tokenizer.decode(output_no_watermark[0], skip_special_tokens=True)[len(prompt):]\n",
    "\n",
    "    # Generate using the soft red list logits processor\n",
    "    output_watermarked = model.generate(\n",
    "        **prompt_ids,\n",
    "        logits_processor=LogitsProcessorList([soft_watermark]),\n",
    "        max_new_tokens=200,\n",
    "        min_new_tokens=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    text_watermarked = tokenizer.decode(output_watermarked[0], skip_special_tokens=True)[len(prompt):]\n",
    "\n",
    "    # Get the average spike entropy during generation\n",
    "    avg_entropy = soft_watermark.get_average_spike_entropy()\n",
    "    # Reset metrics\n",
    "    soft_watermark.reset_metrics()\n",
    "\n",
    "    # Decode and return the string\n",
    "    return {\n",
    "        \"non_watermarked\": {\n",
    "            \"continuation\": text_no_watermark,\n",
    "        },\n",
    "        \"watermarked\": {\n",
    "            \"continuation\": text_watermarked,\n",
    "            \"avg_entropy\": avg_entropy,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5123a2d7",
   "metadata": {},
   "source": [
    "## Running Algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text):\n",
    "    # Tokenize text\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Get sequence length\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    # Use the entire sequence as both input and target\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "\n",
    "    # Calculate loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs.loss * seq_len\n",
    "\n",
    "    # Calculate perplexity\n",
    "    ppl = torch.exp(neg_log_likelihood / seq_len)\n",
    "    return ppl.item()\n",
    "\n",
    "def compute_z_score(green_token_count, total_length, gamma):\n",
    "    numerator = green_token_count - (gamma * total_length)\n",
    "    denominator = math.sqrt(total_length * gamma * (1 - gamma))\n",
    "    return numerator / denominator\n",
    "\n",
    "def print_colored_terminal(text, color_map, tokenizer):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    token_texts = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "    for i, token_text in enumerate(token_texts):\n",
    "        if i < len(color_map):\n",
    "            if color_map[i] == 1:\n",
    "                # Green text in terminal\n",
    "                print(f\"\\033[92m{token_text}\\033[0m\", end=\"\")\n",
    "            else:\n",
    "                # Red text in terminal\n",
    "                print(f\"\\033[91m{token_text}\\033[0m\", end=\"\")\n",
    "        else:\n",
    "            print(token_text, end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c490db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, prompts in enumerate(dataset):\n",
    "  prompt = prompts[\"text\"]\n",
    "  result_dict = {\n",
    "    \"prompt\": prompt,\n",
    "  }\n",
    "  # Tokenize prompt into ids\n",
    "  prompt_ids = tokenize(prompt)\n",
    "  # Generate output\n",
    "  result_dict.update(\n",
    "    algorithm_2(tokenizer, model, prompt_ids)\n",
    "  )\n",
    "\n",
    "  # Given a continuation, compute which tokens are in the red list and which are in the green list\n",
    "  generated_token_ids = tokenize(result_dict[\"watermarked\"][\"continuation\"])\n",
    "  color_map = soft_watermark.produce_color_map(\n",
    "    prompt_ids = prompt_ids,\n",
    "    generated_token_ids = generated_token_ids,\n",
    "  )\n",
    "  continuation_token_count = len(generated_token_ids['input_ids'][0])\n",
    "  green_token_count = len(list(filter(lambda x: x, color_map)))\n",
    "\n",
    "  # Collect results\n",
    "  result_dict[\"watermarked\"] = {\n",
    "    \"continuation\": result_dict[\"watermarked\"][\"continuation\"],\n",
    "    # \"color_map\": color_map,\n",
    "    \"continuation_token_count\": continuation_token_count,\n",
    "    \"green_token_count\": green_token_count,\n",
    "    \"avg_entropy\": result_dict[\"watermarked\"][\"avg_entropy\"],\n",
    "    \"z_score\": compute_z_score(green_token_count, continuation_token_count, gamma),\n",
    "    \"PPL\": compute_perplexity(model, tokenizer, result_dict[\"watermarked\"][\"continuation\"])\n",
    "  }\n",
    "  result_dict[\"non_watermarked\"].update(\n",
    "    {\n",
    "      \"PPL\": compute_perplexity(model, tokenizer, result_dict[\"non_watermarked\"][\"continuation\"])\n",
    "    }\n",
    "  )\n",
    "  results.append(result_dict)\n",
    "  print(json.dumps(result_dict, indent=1))\n",
    "\n",
    "  # For now just break after generating 1 result since it takes a while\n",
    "  if i >= 0:\n",
    "      break\n",
    "\n",
    "# Add this line to print colored text in terminal\n",
    "# print(\"\\nColored continuation:\")\n",
    "# print_colored_terminal(continuation, color_map, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4442e",
   "metadata": {},
   "source": [
    "# Algorithm 3: Robust Private Watermarking\n",
    "\n",
    "![images/algorithm_3.png](images/algorithm_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b69487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code block for Algorithm 3\n",
    "\n",
    "def algorithm_3():\n",
    "    # TODO: Implement Algorithm 3\n",
    "    raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
