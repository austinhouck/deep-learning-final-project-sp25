{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26f68dc",
   "metadata": {},
   "source": [
    "# A Watermark for Large Language Models\n",
    "\n",
    "This is the Python notebook for our project. It generally follows the flow of the original paper, A Watermark for Large Language Models (Kirchenbauer et al. 2023). Much of the text and images in each section is pulled directly from the paper, though we have appended our own code to implement the paper's watermarking strategy and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install block - put any necessary pip installs here\n",
    "!pip install datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fa5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block - put any necessary imports here\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList\n",
    "from functools import partial\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329c439",
   "metadata": {},
   "source": [
    "# Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(F\"Device set to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a181ca0",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "The paper uses the C4 datasetâ€™s RealNewsLike subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"c4\"\n",
    "dataset_config_name = \"realnewslike\"\n",
    "dataset = load_dataset(dataset_name, dataset_config_name, split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705d6d6",
   "metadata": {},
   "source": [
    "# Load the Opt-1.3b tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Helpers\n",
    "def tokenize(sequence):\n",
    "    return tokenizer(sequence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "def seed_rng(seed=42):\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c300f",
   "metadata": {},
   "source": [
    "# Algorithm 1: Text Generation with Hard Red List\n",
    "\n",
    "![images/algorithm_1.png](images/algorithm_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f3a62",
   "metadata": {},
   "source": [
    "## Define the Hard Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardRedListWatermark(LogitsProcessor):\n",
    "\n",
    "    def __init__(self, tokenizer, device, hash_key=15485863):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.green_list_size = self.vocab_size // 2\n",
    "        # Large prime number to be used for seed\n",
    "        self.hash_key = hash_key\n",
    "        self.generator = torch.Generator(device=device)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Compute hash of previous token and set it as seed\n",
    "        prev_token = int(input_ids[0, -1].item())\n",
    "        self.generator.manual_seed(self.hash_key * prev_token)\n",
    "\n",
    "        # Shuffle the vocabulary and get red list ids\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=self.generator, device=input_ids.device)\n",
    "        red_list = permuted_vocab[self.green_list_size:] # continue where green list left off\n",
    "\n",
    "        # Set red list logits to -infinity\n",
    "        scores[:, red_list] = -float(\"inf\")\n",
    "        return scores\n",
    "\n",
    "    def compute_green_and_red_lists(self):\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=self.generator, device=self.device)\n",
    "        green_list = permuted_vocab[:self.green_list_size]\n",
    "        red_list = permuted_vocab[self.green_list_size:]\n",
    "\n",
    "        return green_list, red_list\n",
    "\n",
    "    def produce_color_map(self, prompt_ids, generated_token_ids):\n",
    "        # Set seed for reproducibility\n",
    "        seed_rng()\n",
    "\n",
    "        # Get last prompt_id\n",
    "        prev_token_id = int(prompt_ids['input_ids'][0, -1].item())\n",
    "\n",
    "        color_map = []\n",
    "\n",
    "        for i, token_id in enumerate(generated_token_ids['input_ids'][0]):\n",
    "            token_id = int(token_id.item())\n",
    "            self.generator.manual_seed(self.hash_key * prev_token_id)\n",
    "\n",
    "            green_list, _ = self.compute_green_and_red_lists()\n",
    "\n",
    "            if token_id in green_list:\n",
    "                color_map.append(1) # green\n",
    "            else:\n",
    "                color_map.append(0) # red\n",
    "\n",
    "            # Update previous token before next iteration\n",
    "            prev_token_id = token_id\n",
    "\n",
    "        return color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3aefa",
   "metadata": {},
   "source": [
    "## Implement Algorithm 1 using Hard Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 2.0\n",
    "gamma = 0.5\n",
    "\n",
    "hard_watermark = HardRedListWatermark(tokenizer, model.device)\n",
    "\n",
    "def algorithm_1(tokenizer, model, prompt_ids):\n",
    "    # Set seed for reproducibility\n",
    "    seed_rng()\n",
    "\n",
    "    # Instantiate the hard red list logits processor\n",
    "    hard_red_list_lp = LogitsProcessorList([hard_watermark])\n",
    "\n",
    "    # Generate using the hard red list logits processor\n",
    "    algorithm_1_generate = partial(\n",
    "        model.generate,\n",
    "        logits_processor=hard_red_list_lp,\n",
    "        max_new_tokens=200,\n",
    "        min_new_tokens=1,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Generate output ids\n",
    "    output_ids = algorithm_1_generate(**prompt_ids)\n",
    "\n",
    "    # Decode and return the string\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a71787",
   "metadata": {},
   "source": [
    "## Running Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, prompts in enumerate(dataset):\n",
    "  prompt = prompts[\"text\"]\n",
    "  # Tokenize prompt into ids\n",
    "  prompt_ids = tokenize(prompt)\n",
    "  # Generate output\n",
    "  output = algorithm_1(tokenizer, model, prompt_ids)\n",
    "  continuation = output[len(prompt):]\n",
    "  generated_token_ids = tokenize(continuation)\n",
    "  # Given a continuation, compute which tokens are in the red list and which are in the green list\n",
    "  color_map = hard_watermark.produce_color_map(\n",
    "    prompt_ids = prompt_ids,\n",
    "    generated_token_ids = generated_token_ids,\n",
    "  )\n",
    "  green_token_count = len(list(filter(lambda x: x, color_map)))\n",
    "\n",
    "  # Collect results\n",
    "  result_dict = {\n",
    "    \"prompt\": prompt,\n",
    "    \"continuation\": continuation,\n",
    "    \"full_output\": output,\n",
    "    \"continuation_token_count\": continuation_token_count,\n",
    "    \"green_token_count\": green_token_count,\n",
    "  }\n",
    "  results.append(result_dict)\n",
    "\n",
    "  # For now just break after generating 1 result since it takes a while\n",
    "  if i >= 0:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3ce6e",
   "metadata": {},
   "source": [
    "## Detecting the Hard Watermark\n",
    "\n",
    "While producing watermarked text requires access to the language model, detecting the watermark does not. A third party with knowledge of the hash function and random number generator can re-produce the red list for each token and count how many times the red list rule is violated. We can detect the watermark by testing the following null hypothesis:\n",
    "\n",
    "$H_0$: The text sequence is generated with no knowledge of the red list rule.\n",
    "\n",
    "Because the red list is chosen at random, a natural writer is expected to violate the red list rule with half of their tokens, while the watermarked model produces no violations. The probability that a natural source produces $T$ tokens without violating the red list rule is only $1/(2^T)$, which is vanishingly small even for short text fragments with a dozen words. This enables detection of the watermark (rejection of $H_0$) for, e.g., a synthetic tweet.\n",
    "\n",
    "A more robust detection approach uses a one proportion z-test to evaluate the null hypothesis. If the null hypothesis is true, then the number of green list tokens, denoted $|s|_G$, has expected value $T/2$ and variance $T/4$. The z-statistic for this test is:\n",
    "\n",
    "![images/eq_2.png](images/eq_2.png)\n",
    "\n",
    "We reject the null hypothesis and detect the watermark if z is above a chosen threshold. Suppose we choose to reject the null hypothesis if $z > 4$. In this case, the probability of a false positive is $3 Ã— 10^{âˆ’5}$, which is the one-sided p-value corresponding to $z > 4$. At the same time, we will detect any watermarked sequence with 16 or more tokens (the minimum value of $T$ that produces $z = 4$ when $|s|_G=T$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z_score(green_token_count, total_length, gamma):\n",
    "    numerator = green_token_count - (total_length / 2)\n",
    "    denominator = math.sqrt(total_length)\n",
    "    return 2 * numerator / denominator\n",
    "\n",
    "# Calculate z-score for a given result\n",
    "first_result = results[0]\n",
    "z_score = compute_z_score(first_result[\"green_token_count\"], first_result[\"continuation_token_count\"], gamma)\n",
    "first_result[\"z_score\"] = z_score\n",
    "print(json.dumps(first_result, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2986d",
   "metadata": {},
   "source": [
    "# Algorithm 2: Text Generation with Soft Red List\n",
    "\n",
    "![images/algorithm_2.png](images/algorithm_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f706aa3",
   "metadata": {},
   "source": [
    "## Define the Soft Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbff596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftRedListWatermark(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, device, gamma=0.5, hash_key=15485863, delta=2.0):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.gamma = gamma\n",
    "        self.hash_key = hash_key\n",
    "        self.delta = delta\n",
    "        self.green_list_size = int(self.gamma * self.vocab_size)\n",
    "        self.generator = torch.Generator(device=device)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Compute hash of previous token and set it as seed\n",
    "        prev_token = int(input_ids[0, -1].item())\n",
    "        self.generator.manual_seed(self.hash_key * prev_token)\n",
    "\n",
    "        # Shuffle the vocabulary and get green list ids\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=self.generator, device=input_ids.device)\n",
    "        green_list = permuted_vocab[:self.green_list_size]\n",
    "\n",
    "        # Add delta to green list logits\n",
    "        scores[:, green_list] += self.delta\n",
    "        return scores\n",
    "\n",
    "    def compute_green_and_red_lists(self):\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=self.generator, device=self.device)\n",
    "        green_list = permuted_vocab[:self.green_list_size]\n",
    "        red_list = permuted_vocab[self.green_list_size:]\n",
    "\n",
    "        return green_list, red_list\n",
    "\n",
    "    def produce_color_map(self, prompt_ids, generated_token_ids):\n",
    "        # Set seed for reproducibility\n",
    "        seed_rng()\n",
    "\n",
    "        # Get last prompt_id\n",
    "        prev_token_id = int(prompt_ids['input_ids'][0, -1].item())\n",
    "\n",
    "        color_map = []\n",
    "\n",
    "        for i, token_id in enumerate(generated_token_ids['input_ids'][0]):\n",
    "            token_id = int(token_id.item())\n",
    "            self.generator.manual_seed(self.hash_key * prev_token_id)\n",
    "\n",
    "            green_list, _ = self.compute_green_and_red_lists()\n",
    "\n",
    "            if token_id in green_list:\n",
    "                color_map.append(1) # green\n",
    "            else:\n",
    "                color_map.append(0) # red\n",
    "\n",
    "            # Update previous token before next iteration\n",
    "            prev_token_id = token_id\n",
    "\n",
    "        return color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf1921",
   "metadata": {},
   "source": [
    "## Implement Algorithm 2 using Soft Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 2.0\n",
    "gamma = 0.5\n",
    "\n",
    "soft_watermark = SoftRedListWatermark(tokenizer, model.device)\n",
    "\n",
    "def algorithm_2(tokenizer, model, prompt_ids):\n",
    "    # Set seed for reproducibility\n",
    "    seed_rng()\n",
    "\n",
    "    # Instantiate the soft red list logits processor\n",
    "    soft_red_list_lp = LogitsProcessorList([soft_watermark])\n",
    "\n",
    "    # Generate using the soft red list logits processor\n",
    "    algorithm_2_generate = partial(\n",
    "        model.generate,\n",
    "        logits_processor=soft_red_list_lp,\n",
    "        max_new_tokens=200,\n",
    "        min_new_tokens=1,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Generate output ids\n",
    "    output_ids = algorithm_2_generate(**prompt_ids)\n",
    "\n",
    "    # Decode and return the string\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5123a2d7",
   "metadata": {},
   "source": [
    "## Running Algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def print_colored_terminal(text, color_map, tokenizer):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    token_texts = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "    for i, token_text in enumerate(token_texts):\n",
    "        if i < len(color_map):\n",
    "            if color_map[i] == 1:\n",
    "                # Green text in terminal\n",
    "                print(f\"\\033[92m{token_text}\\033[0m\", end=\"\")\n",
    "            else:\n",
    "                # Red text in terminal\n",
    "                print(f\"\\033[91m{token_text}\\033[0m\", end=\"\")\n",
    "        else:\n",
    "            print(token_text, end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c490db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, prompts in enumerate(dataset):\n",
    "  prompt = prompts[\"text\"]\n",
    "  # Tokenize prompt into ids\n",
    "  prompt_ids = tokenize(prompt)\n",
    "  # Generate output\n",
    "  output = algorithm_2(tokenizer, model, prompt_ids)\n",
    "  continuation = output[len(prompt):]\n",
    "  generated_token_ids = tokenize(continuation)\n",
    "  # Given a continuation, compute which tokens are in the red list and which are in the green list\n",
    "  color_map = soft_watermark.produce_color_map(\n",
    "    prompt_ids = prompt_ids,\n",
    "    generated_token_ids = generated_token_ids,\n",
    "  )\n",
    "  green_token_count = len(list(filter(lambda x: x, color_map)))\n",
    "\n",
    "  # Collect results\n",
    "  result_dict = {\n",
    "    \"prompt\": prompt,\n",
    "    \"continuation\": continuation,\n",
    "    \"full_output\": output,\n",
    "    \"continuation_token_count\": continuation_token_count,\n",
    "    \"green_token_count\": green_token_count,\n",
    "  }\n",
    "  results.append(result_dict)\n",
    "\n",
    "  # For now just break after generating 1 result since it takes a while\n",
    "  if i >= 0:\n",
    "      break\n",
    "\n",
    "# Add this line to print colored text in terminal\n",
    "# print(\"\\nColored continuation:\")\n",
    "# print_colored_terminal(continuation, color_map, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e92bdd",
   "metadata": {},
   "source": [
    "## Detecting the Soft Watermark\n",
    "\n",
    "The process for detecting the soft watermark is identical to that for the hard watermark. We assume the null hypothesis:\n",
    "\n",
    "$H_0$: The text sequence is generated with no knowledge of the red list rule.\n",
    "\n",
    "and compute a z-statistic using:\n",
    "\n",
    "![images/eq_2.png](images/eq_2.png)\n",
    "\n",
    "We reject the null hypothesis and detect the watermark if z is greater than a threshold. For arbitrary $\\gamma$ we have\n",
    "\n",
    "![images/eq_3.png](images/eq_3.png)\n",
    "\n",
    "Consider again the case in which we detect the watermark for $z > 4$. Just like in the case of the hard watermark, we get false positives with rate $3 Ã— 10^{âˆ’5}$. In the case of the hard watermark, we could detect any watermarked sequence of length 16 tokens or more, regardless of the properties of the text. However, in the case of the soft watermark our ability to detect synthetic text depends on the entropy of the sequence. High entropy sequences are detected with relatively few tokens, while low entropy sequences require more tokens for detection. Below, we rigorously analyze the detection sensitivity of the soft watermark, and its dependence on entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z_score(green_token_count, total_length, gamma):\n",
    "    numerator = green_token_count - (gamma * total_length)\n",
    "    denominator = math.sqrt(total_length * gamma * (1 - gamma))\n",
    "    return numerator / denominator\n",
    "\n",
    "# Calculate z-score for a given result\n",
    "first_result = results[0]\n",
    "z_score = compute_z_score(first_result[\"green_token_count\"], first_result[\"continuation_token_count\"], gamma)\n",
    "first_result[\"z_score\"] = z_score\n",
    "print(json.dumps(first_result, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978a77f1",
   "metadata": {},
   "source": [
    "## Define Spike Entropy\n",
    "\n",
    "![images/spike_entropy.png](images/spike_entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f685a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_spike_entropy(p, z):\n",
    "    denom = 1.0 + z * p\n",
    "    denom = torch.clamp(denom, min=1e-9) # Prevent division by zero/very small numbers\n",
    "    return torch.sum(p / denom, dim=-1) # Sum across the vocabulary dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4442e",
   "metadata": {},
   "source": [
    "# Algorithm 3: Robust Private Watermarking\n",
    "\n",
    "![images/algorithm_3.png](images/algorithm_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b69487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code block for Algorithm 3\n",
    "\n",
    "def algorithm_3():\n",
    "    # TODO: Implement Algorithm 3\n",
    "    raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
