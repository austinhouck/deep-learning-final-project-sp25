{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26f68dc",
   "metadata": {},
   "source": [
    "# A Watermark for Large Language Models\n",
    "\n",
    "This is the Python notebook for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install block - put any necessary pip installs here\n",
    "!pip install datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fa5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block - put any necessary imports here\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList\n",
    "from functools import partial\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329c439",
   "metadata": {},
   "source": [
    "Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(F\"Device set to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a181ca0",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "The paper uses the C4 datasetâ€™s RealNewsLike subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"c4\"\n",
    "dataset_config_name = \"realnewslike\"\n",
    "dataset = load_dataset(dataset_name, dataset_config_name, split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705d6d6",
   "metadata": {},
   "source": [
    "# Load the Opt-1.3b tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c300f",
   "metadata": {},
   "source": [
    "# Algorithm 1: Text Generation with Hard Red List\n",
    "\n",
    "![algorithm_1.png](algorithm_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f3a62",
   "metadata": {},
   "source": [
    "### Define the Hard Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardRedList(LogitsProcessor):\n",
    "\n",
    "    def __init__(self, vocab_size, hash_key=15485863):\n",
    "      self.vocab_size = vocab_size\n",
    "      # Large prime number to be used for seed\n",
    "      self.hash_key = hash_key\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Compute hash of previous token and set it as seed\n",
    "        prev_token = int(input_ids[0, -1].item())\n",
    "        g = torch.Generator(device=input_ids.device)\n",
    "        g.manual_seed(self.hash_key * prev_token)\n",
    "\n",
    "        # Red list size is half of the vocabulary size\n",
    "        redlist_size = self.vocab_size // 2\n",
    "\n",
    "        # Shuffle the vocabulary and get red list ids\n",
    "        vocab_permutation = torch.randperm(self.vocab_size, generator=g, device=input_ids.device)\n",
    "        redlist_ids = vocab_permutation[:redlist_size]\n",
    "\n",
    "        # Set redlist logits to -infinity\n",
    "        scores[:, list(redlist_ids)] = -float(\"inf\")\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3aefa",
   "metadata": {},
   "source": [
    "### Implement Algorithm 1 using Hard Red List Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code block for Algorithm 1\n",
    "\n",
    "def algorithm_1(tokenizer, model, prompt):\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "    # Instantiate the hard red list logits processor\n",
    "    hard_red_list_lp = LogitsProcessorList([HardRedList(tokenizer.vocab_size)])\n",
    "\n",
    "    # Generate using the hard red list logits processor\n",
    "    algorithm_1_generate = partial(\n",
    "        model.generate,\n",
    "        logits_processor=hard_red_list_lp,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Tokenize prompt into ids\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate output ids\n",
    "    output_ids = algorithm_1_generate(**prompt_ids)\n",
    "\n",
    "    # Decode and return the string\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a71787",
   "metadata": {},
   "source": [
    "### Running Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prompts in enumerate(dataset):\n",
    "  prompt = prompts[\"text\"]\n",
    "  output = algorithm_1(tokenizer, model, prompt)\n",
    "  continuation = output[len(prompt):]\n",
    "  result_dict = {\n",
    "    \"prompt\": prompt,\n",
    "    \"continuation\": continuation,\n",
    "    \"full_output\": output,\n",
    "  }\n",
    "  print(json.dumps(result_dict, indent=1))\n",
    "\n",
    "  # For now just break after generating 1 result since it takes a while\n",
    "  if i >= 0:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2986d",
   "metadata": {},
   "source": [
    "# Algorithm 2: Text Generation with Soft Red List\n",
    "\n",
    "![algorithm_2.png](algorithm_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbff596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftWatermark(LogitsProcessor):\n",
    "    def __init__(self, vocab_size, gamma=0.5, hash_key=15485863, delta=2.0 ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.gamma = gamma\n",
    "        self.hash_key = hash_key\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        prev_token = int(input_ids[0,-1].item())\n",
    "        g = torch.Generator(device=input_ids.device) #want a local generator not global\n",
    "        g.manual_seed(self.hash_key * prev_token) #this is fixed so that the seed is deterministic\n",
    "        \n",
    "        green_list_size = int(self.gamma * self.vocab_size)\n",
    "        permuted_vocab = torch.randperm(self.vocab_size, generator=g, device=input_ids.device)\n",
    "        green_list = permuted_vocab[:green_list_size]\n",
    "\n",
    "        scores[:, green_list] += self.delta\n",
    "        return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code block for Algorithm 2\n",
    "\n",
    "def algorithm_2(tokenizer, model, prompt):\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "    # Instantiate the hard red list logits processor\n",
    "    hard_red_list_lp = LogitsProcessorList([SoftWatermark(tokenizer.vocab_size)])\n",
    "\n",
    "    # Generate using the hard red list logits processor\n",
    "    algorithm_2_generate = partial(\n",
    "        model.generate,\n",
    "        logits_processor=soft_lp,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Tokenize prompt into ids\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate output ids\n",
    "    output_ids = algorithm_2_generate(**prompt_ids)\n",
    "\n",
    "    # Decode and return the string\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4442e",
   "metadata": {},
   "source": [
    "# Algorithm 3: Robust Private Watermarking\n",
    "\n",
    "![algorithm_3.png](algorithm_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b69487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code block for Algorithm 3\n",
    "\n",
    "def algorithm_3():\n",
    "    # TODO: Implement Algorithm 3\n",
    "    raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
